{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In this project I try to classify handwritten Persian numbers by MLP method. The data is freely available in: ‫‪https://users.encs.concordia.ca/~j_sadri/PersianDatabase.htm‬‬ ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### By using the following code I go through image files and resize thier size into 20*20 pixel and by using Image.open.convert I make a vector from each image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.ndimage import convolve\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.utils import shuffle\n",
    "import os.path\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from PIL import Image\n",
    "import glob, os\n",
    "from PIL import Image\n",
    "from resizeimage import resizeimage\n",
    "\n",
    "size = 20, 20\n",
    "\n",
    "for filename in glob.glob(\"./HW2-PersianDigits-resized/Test/8/*.tif\"):\n",
    "    fd_img = open(filename, 'rb')\n",
    "    img = Image.open(fd_img)\n",
    "    img = resizeimage.resize_contain(img, [20, 20])\n",
    "    img.save(filename, img.format)\n",
    "    fd_img.close()\n",
    "\n",
    "Featur_8_array = np.zeros(400)\n",
    "for filename in glob.glob(\"./HW2-PersianDigits-resized/Test/8/*.tif\"):\n",
    "    fd_img = open(filename, 'rb')\n",
    "    img = Image.open(fd_img).convert('L')\n",
    "    arr = np.array(img)\n",
    "    shape = arr.shape\n",
    "\n",
    "    flat_arr = arr.ravel()\n",
    "\n",
    "    vector = np.matrix(flat_arr)\n",
    "\n",
    "    arr2 = np.asarray(vector).reshape(shape)\n",
    "\n",
    "    img2 = Image.fromarray(arr2, 'L')\n",
    "    Featur_8_array = np.vstack((Featur_8_array, flat_arr))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Then by using the following code I make a dictionary from the Image vectors and stick label to vector of each number (same prosiger for Training and Test set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dic = {'data':Featur_8_array, 'label': 8*np.ones(6)}\n",
    "test_dic['label']\n",
    "np.save('Featur_8_test.npy', test_dic) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Then I merge all the produced dictionaries for Test and Training sets to the two following dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_dictionary0 = np.load('Featur_0.npy').item()\n",
    "read_dictionary1 = np.load('Featur_1.npy').item()\n",
    "read_dictionary2 = np.load('Featur_2.npy').item()\n",
    "read_dictionary3 = np.load('Featur_3.npy').item()\n",
    "read_dictionary4 = np.load('Featur_4.npy').item()\n",
    "read_dictionary5 = np.load('Featur_5.npy').item()\n",
    "read_dictionary6 = np.load('Featur_6.npy').item()\n",
    "read_dictionary7 = np.load('Featur_7.npy').item()\n",
    "read_dictionary8 = np.load('Featur_8.npy').item()\n",
    "read_dictionary9 = np.load('Featur_9.npy').item()\n",
    "\n",
    "dic_tot = {}\n",
    "for key in read_dictionary0.keys():\n",
    "    dic_tot[key] = np.vstack((read_dictionary0[key][1:], read_dictionary1[key][1:], read_dictionary2[key][1:] , read_dictionary3[key][1:]\\\n",
    "    ,read_dictionary4[key][1:],read_dictionary5[key][1:],read_dictionary6[key][1:],read_dictionary7[key][1:],read_dictionary8[key][1:],read_dictionary9[key][1:] ))\n",
    "\n",
    "\n",
    "\n",
    "read_dictionary0_t = np.load('Featur_0_test.npy').item()\n",
    "read_dictionary1_t = np.load('Featur_1_test.npy').item()\n",
    "read_dictionary2_t = np.load('Featur_2_test.npy').item()\n",
    "read_dictionary3_t = np.load('Featur_3_test.npy').item()\n",
    "read_dictionary4_t = np.load('Featur_4_test.npy').item()\n",
    "read_dictionary5_t = np.load('Featur_5_test.npy').item()\n",
    "read_dictionary6_t = np.load('Featur_6_test.npy').item()\n",
    "read_dictionary7_t = np.load('Featur_7_test.npy').item()\n",
    "read_dictionary8_t = np.load('Featur_8_test.npy').item()\n",
    "read_dictionary9_t = np.load('Featur_9_test.npy').item()\n",
    "\n",
    "dic_tot_test = {}\n",
    "for key in read_dictionary0_t.keys():\n",
    "    dic_tot_test[key] = np.vstack((read_dictionary0_t[key][1:], read_dictionary1_t[key][1:], read_dictionary2_t[key][1:] , read_dictionary3_t[key][1:]\\\n",
    "    ,read_dictionary4_t[key][1:],read_dictionary5_t[key][1:],read_dictionary6_t[key][1:],read_dictionary7_t[key][1:],read_dictionary8_t[key][1:],read_dictionary9_t[key][1:] ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #  In the following part I train the system for  one layer with 200 neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got MNIST with 150 training- and 50 test samples\n",
      "Loading model from file.\n",
      "Test accuracy: 0.24\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "PATH = 'mlp_model_200'\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    X, y = dic_tot['data'], dic_tot['label'].ravel()\n",
    "    X2, y2 = shuffle(X, y, random_state=0)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X2 / 255., y2, test_size=0.25)\n",
    "\n",
    "    print('Got MNIST with %d training- and %d test samples' % (len(y_train), len(y_test)))\n",
    "    #print('Digit distribution in whole dataset:', np.bincount(y2.astype('int32')))\n",
    "\n",
    "    clf = None\n",
    "    if os.path.exists(PATH):\n",
    "        print('Loading model from file.')\n",
    "        clf = joblib.load(PATH).best_estimator_\n",
    "    else:\n",
    "        print('Training model.')\n",
    "        # params = {'hidden_layer_sizes': [(256,), (512,), (128, 256, 128,)]}\n",
    "        params = {'hidden_layer_sizes': [(200,)]}\n",
    "        mlp = MLPClassifier(verbose=10, momentum=0.9, learning_rate='adaptive', activation='relu')\n",
    "        clf = GridSearchCV(mlp, params, verbose=10, n_jobs=-1, cv=5)\n",
    "        clf.fit(X_train, y_train)\n",
    "        print('Finished with grid search with best mean cross-validated score:', clf.best_score_)\n",
    "        # print('Best params appeared to be', clf.best_params_)\n",
    "        joblib.dump(clf, PATH)\n",
    "        clf = clf.best_estimator_\n",
    "\n",
    "    print('Test accuracy:', clf.score(X_test, y_test))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # After traing the system I calculate the confusion matrix and accuracy of the Test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " The confusion matrix is:\n",
      " [[5 0 0 0 0 0 0 0 0 0]\n",
      " [0 5 0 0 0 0 0 0 0 0]\n",
      " [0 0 4 1 0 0 0 0 0 0]\n",
      " [0 0 0 4 1 0 0 0 0 0]\n",
      " [1 0 1 0 3 0 0 0 0 0]\n",
      " [0 0 0 0 0 5 0 0 0 0]\n",
      " [1 0 0 1 0 0 2 1 0 0]\n",
      " [0 0 0 0 0 0 0 5 0 0]\n",
      " [0 0 0 0 0 2 0 0 3 0]\n",
      " [0 0 0 0 0 0 0 0 0 5]]\n",
      "\n",
      " The accuracy is: 0.16\n"
     ]
    }
   ],
   "source": [
    "X, y = dic_tot_test['data'], dic_tot_test['label'].ravel()\n",
    "X, y = shuffle(X, y, random_state=0)\n",
    "X_test=X \n",
    "y_test=y \n",
    "\n",
    "print('\\n The confusion matrix is:\\n' ,confusion_matrix(y_test,prediction))\n",
    "print( '\\n The accuracy is:', clf.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # One layer with 600 neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got MNIST with 150 training- and 50 test samples\n",
      "Loading model from file.\n",
      "Test accuracy: 0.94\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "PATH = 'mlp_model_600'\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    X, y = dic_tot['data'], dic_tot['label'].ravel()\n",
    "    X2, y2 = shuffle(X, y, random_state=0)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X2 / 255., y2, test_size=0.25)\n",
    "\n",
    "    print('Got MNIST with %d training- and %d test samples' % (len(y_train), len(y_test)))\n",
    "    #print('Digit distribution in whole dataset:', np.bincount(y2.astype('int32')))\n",
    "\n",
    "    clf = None\n",
    "    if os.path.exists(PATH):\n",
    "        print('Loading model from file.')\n",
    "        clf = joblib.load(PATH).best_estimator_\n",
    "    else:\n",
    "        print('Training model.')\n",
    "        # params = {'hidden_layer_sizes': [(256,), (512,), (128, 256, 128,)]}\n",
    "        params = {'hidden_layer_sizes': [(600,)]}\n",
    "        mlp = MLPClassifier(verbose=10, momentum=0.9, learning_rate='adaptive', activation='relu')\n",
    "        clf = GridSearchCV(mlp, params, verbose=10, n_jobs=-1, cv=5)\n",
    "        clf.fit(X_train, y_train)\n",
    "        print('Finished with grid search with best mean cross-validated score:', clf.best_score_)\n",
    "        # print('Best params appeared to be', clf.best_params_)\n",
    "        joblib.dump(clf, PATH)\n",
    "        clf = clf.best_estimator_\n",
    "\n",
    "    print('Test accuracy:', clf.score(X_test, y_test))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " The confusion matrix is:\n",
      " [[5 0 0 0 0 0 0 0 0 0]\n",
      " [0 5 0 0 0 0 0 0 0 0]\n",
      " [0 0 4 1 0 0 0 0 0 0]\n",
      " [0 0 0 4 1 0 0 0 0 0]\n",
      " [1 0 1 0 3 0 0 0 0 0]\n",
      " [0 0 0 0 0 5 0 0 0 0]\n",
      " [1 0 0 1 0 0 2 1 0 0]\n",
      " [0 0 0 0 0 0 0 5 0 0]\n",
      " [0 0 0 0 0 2 0 0 3 0]\n",
      " [0 0 0 0 0 0 0 0 0 5]]\n",
      "\n",
      " The accuracy is: 0.68\n"
     ]
    }
   ],
   "source": [
    "X, y = dic_tot_test['data'], dic_tot_test['label'].ravel()\n",
    "X, y = shuffle(X, y, random_state=0)\n",
    "X_test=X \n",
    "y_test=y \n",
    "\n",
    "print('\\n The confusion matrix is:\\n' ,confusion_matrix(y_test,prediction))\n",
    "print( '\\n The accuracy is:', clf.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Two layer with 500 and 300 neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got MNIST with 150 training- and 50 test samples\n",
      "Loading model from file.\n",
      "Test accuracy: 0.92\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "PATH = 'mlp_model_2Layer'\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    X, y = dic_tot['data'], dic_tot['label'].ravel()\n",
    "    X2, y2 = shuffle(X, y, random_state=0)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X2 / 255., y2, test_size=0.25)\n",
    "\n",
    "    print('Got MNIST with %d training- and %d test samples' % (len(y_train), len(y_test)))\n",
    "    #print('Digit distribution in whole dataset:', np.bincount(y2.astype('int32')))\n",
    "\n",
    "    clf = None\n",
    "    if os.path.exists(PATH):\n",
    "        print('Loading model from file.')\n",
    "        clf = joblib.load(PATH).best_estimator_\n",
    "    else:\n",
    "        print('Training model.')\n",
    "        # params = {'hidden_layer_sizes': [(256,), (512,), (128, 256, 128,)]}\n",
    "        params = {'hidden_layer_sizes': [(500,300)]}\n",
    "        mlp = MLPClassifier(verbose=10, momentum=0.9, learning_rate='adaptive', activation='relu')\n",
    "        clf = GridSearchCV(mlp, params, verbose=10, n_jobs=-1, cv=5)\n",
    "        clf.fit(X_train, y_train)\n",
    "        print('Finished with grid search with best mean cross-validated score:', clf.best_score_)\n",
    "        # print('Best params appeared to be', clf.best_params_)\n",
    "        joblib.dump(clf, PATH)\n",
    "        clf = clf.best_estimator_\n",
    "\n",
    "    print('Test accuracy:', clf.score(X_test, y_test))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " The confusion matrix is:\n",
      " [[5 0 0 0 0 0 0 0 0 0]\n",
      " [0 5 0 0 0 0 0 0 0 0]\n",
      " [0 0 4 1 0 0 0 0 0 0]\n",
      " [0 0 0 4 1 0 0 0 0 0]\n",
      " [1 0 1 0 3 0 0 0 0 0]\n",
      " [0 0 0 0 0 5 0 0 0 0]\n",
      " [1 0 0 1 0 0 2 1 0 0]\n",
      " [0 0 0 0 0 0 0 5 0 0]\n",
      " [0 0 0 0 0 2 0 0 3 0]\n",
      " [0 0 0 0 0 0 0 0 0 5]]\n",
      "\n",
      " The accuracy is: 0.74\n"
     ]
    }
   ],
   "source": [
    "X, y = dic_tot_test['data'], dic_tot_test['label'].ravel()\n",
    "X, y = shuffle(X, y, random_state=0)\n",
    "X_test=X \n",
    "y_test=y \n",
    "\n",
    "print('\\n The confusion matrix is:\\n' ,confusion_matrix(y_test,prediction))\n",
    "print( '\\n The accuracy is:', clf.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # One layer with 50 neurons\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got MNIST with 150 training- and 50 test samples\n",
      "Loading model from file.\n",
      "Test accuracy: 0.9\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "PATH = 'mlp_model_50'\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    X, y = dic_tot['data'], dic_tot['label'].ravel()\n",
    "    X2, y2 = shuffle(X, y, random_state=0)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X2 / 255., y2, test_size=0.25)\n",
    "\n",
    "    print('Got MNIST with %d training- and %d test samples' % (len(y_train), len(y_test)))\n",
    "    #print('Digit distribution in whole dataset:', np.bincount(y2.astype('int32')))\n",
    "\n",
    "    clf = None\n",
    "    if os.path.exists(PATH):\n",
    "        print('Loading model from file.')\n",
    "        clf = joblib.load(PATH).best_estimator_\n",
    "    else:\n",
    "        print('Training model.')\n",
    "        # params = {'hidden_layer_sizes': [(256,), (512,), (128, 256, 128,)]}\n",
    "        params = {'hidden_layer_sizes': [(50,)]}\n",
    "        mlp = MLPClassifier(verbose=10, momentum=0.9, learning_rate='adaptive', activation='relu')\n",
    "        clf = GridSearchCV(mlp, params, verbose=10, n_jobs=-1, cv=5)\n",
    "        clf.fit(X_train, y_train)\n",
    "        print('Finished with grid search with best mean cross-validated score:', clf.best_score_)\n",
    "        # print('Best params appeared to be', clf.best_params_)\n",
    "        joblib.dump(clf, PATH)\n",
    "        clf = clf.best_estimator_\n",
    "\n",
    "    print('Test accuracy:', clf.score(X_test, y_test))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " The confusion matrix is:\n",
      " [[5 0 0 0 0 0 0 0 0 0]\n",
      " [0 5 0 0 0 0 0 0 0 0]\n",
      " [0 0 4 1 0 0 0 0 0 0]\n",
      " [0 0 0 4 1 0 0 0 0 0]\n",
      " [1 0 1 0 3 0 0 0 0 0]\n",
      " [0 0 0 0 0 5 0 0 0 0]\n",
      " [1 0 0 1 0 0 2 1 0 0]\n",
      " [0 0 0 0 0 0 0 5 0 0]\n",
      " [0 0 0 0 0 2 0 0 3 0]\n",
      " [0 0 0 0 0 0 0 0 0 5]]\n",
      "\n",
      " The accuracy is: 0.7\n"
     ]
    }
   ],
   "source": [
    "X, y = dic_tot_test['data'], dic_tot_test['label'].ravel()\n",
    "X, y = shuffle(X, y, random_state=0)\n",
    "X_test=X \n",
    "y_test=y \n",
    "\n",
    "print('\\n The confusion matrix is:\\n' ,confusion_matrix(y_test,prediction))\n",
    "print( '\\n The accuracy is:', clf.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Two layer with 50 and 25 neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got MNIST with 150 training- and 50 test samples\n",
      "Loading model from file.\n",
      "Test accuracy: 0.94\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "PATH = 'mlp_model_2Layer_2'\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    X, y = dic_tot['data'], dic_tot['label'].ravel()\n",
    "    X2, y2 = shuffle(X, y, random_state=0)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X2 / 255., y2, test_size=0.25)\n",
    "\n",
    "    print('Got MNIST with %d training- and %d test samples' % (len(y_train), len(y_test)))\n",
    "    #print('Digit distribution in whole dataset:', np.bincount(y2.astype('int32')))\n",
    "\n",
    "    clf = None\n",
    "    if os.path.exists(PATH):\n",
    "        print('Loading model from file.')\n",
    "        clf = joblib.load(PATH).best_estimator_\n",
    "    else:\n",
    "        print('Training model.')\n",
    "        # params = {'hidden_layer_sizes': [(256,), (512,), (128, 256, 128,)]}\n",
    "        params = {'hidden_layer_sizes': [(50,25)]}\n",
    "        mlp = MLPClassifier(verbose=10, momentum=0.9, learning_rate='adaptive', activation='relu')\n",
    "        clf = GridSearchCV(mlp, params, verbose=10, n_jobs=-1, cv=5)\n",
    "        clf.fit(X_train, y_train)\n",
    "        print('Finished with grid search with best mean cross-validated score:', clf.best_score_)\n",
    "        # print('Best params appeared to be', clf.best_params_)\n",
    "        joblib.dump(clf, PATH)\n",
    "        clf = clf.best_estimator_\n",
    "\n",
    "    print('Test accuracy:', clf.score(X_test, y_test))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " The confusion matrix is:\n",
      " [[5 0 0 0 0 0 0 0 0 0]\n",
      " [0 5 0 0 0 0 0 0 0 0]\n",
      " [0 0 4 1 0 0 0 0 0 0]\n",
      " [0 0 0 4 1 0 0 0 0 0]\n",
      " [1 0 1 0 3 0 0 0 0 0]\n",
      " [0 0 0 0 0 5 0 0 0 0]\n",
      " [1 0 0 1 0 0 2 1 0 0]\n",
      " [0 0 0 0 0 0 0 5 0 0]\n",
      " [0 0 0 0 0 2 0 0 3 0]\n",
      " [0 0 0 0 0 0 0 0 0 5]]\n",
      "\n",
      " The accuracy is: 0.7\n"
     ]
    }
   ],
   "source": [
    "X, y = dic_tot_test['data'], dic_tot_test['label'].ravel()\n",
    "X, y = shuffle(X, y, random_state=0)\n",
    "X_test=X \n",
    "y_test=y \n",
    "\n",
    "print('\\n The confusion matrix is:\\n' ,confusion_matrix(y_test,prediction))\n",
    "print( '\\n The accuracy is:', clf.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I have imported the following cell to show how to load a previously learned model and use it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.52443136\n",
      "Iteration 2, loss = 2.34559998\n",
      "Iteration 3, loss = 2.27130026\n",
      "Iteration 4, loss = 2.27782508\n",
      "Iteration 5, loss = 2.27198893\n",
      "Iteration 6, loss = 2.24670804\n",
      "Iteration 7, loss = 2.20838077\n",
      "Iteration 8, loss = 2.18289159\n",
      "Iteration 9, loss = 2.17751861\n",
      "Iteration 10, loss = 2.16989941\n",
      "Iteration 11, loss = 2.15630954\n",
      "Iteration 12, loss = 2.13848831\n",
      "Iteration 13, loss = 2.12122351\n",
      "Iteration 14, loss = 2.10543111\n",
      "Iteration 15, loss = 2.09079174\n",
      "Iteration 16, loss = 2.07657041\n",
      "Iteration 17, loss = 2.06215777\n",
      "Iteration 18, loss = 2.04713199\n",
      "Iteration 19, loss = 2.03294542\n",
      "Iteration 20, loss = 2.01879449\n",
      "Iteration 21, loss = 2.00392343\n",
      "Iteration 22, loss = 1.99160373\n",
      "Iteration 23, loss = 1.97546755\n",
      "Iteration 24, loss = 1.95919945\n",
      "Iteration 25, loss = 1.94431207\n",
      "Iteration 26, loss = 1.92674342\n",
      "Iteration 27, loss = 1.90997489\n",
      "Iteration 28, loss = 1.89606094\n",
      "Iteration 29, loss = 1.88147772\n",
      "Iteration 30, loss = 1.86413279\n",
      "Iteration 31, loss = 1.84792082\n",
      "Iteration 32, loss = 1.83234321\n",
      "Iteration 33, loss = 1.81520251\n",
      "Iteration 34, loss = 1.79922829\n",
      "Iteration 35, loss = 1.78204482\n",
      "Iteration 36, loss = 1.76486301\n",
      "Iteration 37, loss = 1.74717418\n",
      "Iteration 38, loss = 1.73001907\n",
      "Iteration 39, loss = 1.71252541\n",
      "Iteration 40, loss = 1.69566220\n",
      "Iteration 41, loss = 1.67751823\n",
      "Iteration 42, loss = 1.65883369\n",
      "Iteration 43, loss = 1.64112102\n",
      "Iteration 44, loss = 1.62297557\n",
      "Iteration 45, loss = 1.60463236\n",
      "Iteration 46, loss = 1.58605677\n",
      "Iteration 47, loss = 1.56730314\n",
      "Iteration 48, loss = 1.54836653\n",
      "Iteration 49, loss = 1.52913301\n",
      "Iteration 50, loss = 1.50991642\n",
      "Iteration 51, loss = 1.49089365\n",
      "Iteration 52, loss = 1.47123300\n",
      "Iteration 53, loss = 1.45194107\n",
      "Iteration 54, loss = 1.43213032\n",
      "Iteration 55, loss = 1.41171431\n",
      "Iteration 56, loss = 1.39213132\n",
      "Iteration 57, loss = 1.37173130\n",
      "Iteration 58, loss = 1.35200866\n",
      "Iteration 59, loss = 1.33235303\n",
      "Iteration 60, loss = 1.31194074\n",
      "Iteration 61, loss = 1.29217016\n",
      "Iteration 62, loss = 1.27183684\n",
      "Iteration 63, loss = 1.25214539\n",
      "Iteration 64, loss = 1.23254764\n",
      "Iteration 65, loss = 1.21223332\n",
      "Iteration 66, loss = 1.19311675\n",
      "Iteration 67, loss = 1.17339592\n",
      "Iteration 68, loss = 1.15416371\n",
      "Iteration 69, loss = 1.13447503\n",
      "Iteration 70, loss = 1.11568548\n",
      "Iteration 71, loss = 1.09585123\n",
      "Iteration 72, loss = 1.07731683\n",
      "Iteration 73, loss = 1.05821455\n",
      "Iteration 74, loss = 1.03954350\n",
      "Iteration 75, loss = 1.02120769\n",
      "Iteration 76, loss = 1.00287595\n",
      "Iteration 77, loss = 0.98474784\n",
      "Iteration 78, loss = 0.96680334\n",
      "Iteration 79, loss = 0.94897199\n",
      "Iteration 80, loss = 0.93181928\n",
      "Iteration 81, loss = 0.91426366\n",
      "Iteration 82, loss = 0.89698551\n",
      "Iteration 83, loss = 0.88019112\n",
      "Iteration 84, loss = 0.86318826\n",
      "Iteration 85, loss = 0.84662839\n",
      "Iteration 86, loss = 0.83013041\n",
      "Iteration 87, loss = 0.81392064\n",
      "Iteration 88, loss = 0.79783161\n",
      "Iteration 89, loss = 0.78239159\n",
      "Iteration 90, loss = 0.76680175\n",
      "Iteration 91, loss = 0.75144530\n",
      "Iteration 92, loss = 0.73673630\n",
      "Iteration 93, loss = 0.72188145\n",
      "Iteration 94, loss = 0.70755661\n",
      "Iteration 95, loss = 0.69339081\n",
      "Iteration 96, loss = 0.67962016\n",
      "Iteration 97, loss = 0.66579366\n",
      "Iteration 98, loss = 0.65256844\n",
      "Iteration 99, loss = 0.63946596\n",
      "Iteration 100, loss = 0.62651652\n",
      "Iteration 101, loss = 0.61369068\n",
      "Iteration 102, loss = 0.60092992\n",
      "Iteration 103, loss = 0.58842484\n",
      "Iteration 104, loss = 0.57581553\n",
      "Iteration 105, loss = 0.56378311\n",
      "Iteration 106, loss = 0.55183331\n",
      "Iteration 107, loss = 0.54032122\n",
      "Iteration 108, loss = 0.52888448\n",
      "Iteration 109, loss = 0.51790546\n",
      "Iteration 110, loss = 0.50705667\n",
      "Iteration 111, loss = 0.49624063\n",
      "Iteration 112, loss = 0.48588886\n",
      "Iteration 113, loss = 0.47557337\n",
      "Iteration 114, loss = 0.46602850\n",
      "Iteration 115, loss = 0.45508875\n",
      "Iteration 116, loss = 0.44622009\n",
      "Iteration 117, loss = 0.43593699\n",
      "Iteration 118, loss = 0.42787217\n",
      "Iteration 119, loss = 0.41804224\n",
      "Iteration 120, loss = 0.40925245\n",
      "Iteration 121, loss = 0.40043807\n",
      "Iteration 122, loss = 0.39218887\n",
      "Iteration 123, loss = 0.38349830\n",
      "Iteration 124, loss = 0.37460099\n",
      "Iteration 125, loss = 0.36767327\n",
      "Iteration 126, loss = 0.35795990\n",
      "Iteration 127, loss = 0.35146955\n",
      "Iteration 128, loss = 0.34340065\n",
      "Iteration 129, loss = 0.33493060\n",
      "Iteration 130, loss = 0.32798469\n",
      "Iteration 131, loss = 0.32001677\n",
      "Iteration 132, loss = 0.31346546\n",
      "Iteration 133, loss = 0.30566923\n",
      "Iteration 134, loss = 0.29939155\n",
      "Iteration 135, loss = 0.29184709\n",
      "Iteration 136, loss = 0.28577582\n",
      "Iteration 137, loss = 0.27947404\n",
      "Iteration 138, loss = 0.27223557\n",
      "Iteration 139, loss = 0.26661982\n",
      "Iteration 140, loss = 0.25992552\n",
      "Iteration 141, loss = 0.25411158\n",
      "Iteration 142, loss = 0.24812583\n",
      "Iteration 143, loss = 0.24262203\n",
      "Iteration 144, loss = 0.23687600\n",
      "Iteration 145, loss = 0.23128547\n",
      "Iteration 146, loss = 0.22613687\n",
      "Iteration 147, loss = 0.22079561\n",
      "Iteration 148, loss = 0.21579086\n",
      "Iteration 149, loss = 0.21064136\n",
      "Iteration 150, loss = 0.20563048\n",
      "Iteration 151, loss = 0.20110452\n",
      "Iteration 152, loss = 0.19610109\n",
      "Iteration 153, loss = 0.19166818\n",
      "Iteration 154, loss = 0.18711067\n",
      "Iteration 155, loss = 0.18276409\n",
      "Iteration 156, loss = 0.17833787\n",
      "Iteration 157, loss = 0.17417043\n",
      "Iteration 158, loss = 0.17005933\n",
      "Iteration 159, loss = 0.16606025\n",
      "Iteration 160, loss = 0.16213393\n",
      "Iteration 161, loss = 0.15832289\n",
      "Iteration 162, loss = 0.15460917\n",
      "Iteration 163, loss = 0.15101599\n",
      "Iteration 164, loss = 0.14745036\n",
      "Iteration 165, loss = 0.14402097\n",
      "Iteration 166, loss = 0.14059716\n",
      "Iteration 167, loss = 0.13735299\n",
      "Iteration 168, loss = 0.13415309\n",
      "Iteration 169, loss = 0.13105001\n",
      "Iteration 170, loss = 0.12797981\n",
      "Iteration 171, loss = 0.12497253\n",
      "Iteration 172, loss = 0.12213921\n",
      "Iteration 173, loss = 0.11931891\n",
      "Iteration 174, loss = 0.11659740\n",
      "Iteration 175, loss = 0.11391379\n",
      "Iteration 176, loss = 0.11131225\n",
      "Iteration 177, loss = 0.10883476\n",
      "Iteration 178, loss = 0.10652841\n",
      "Iteration 179, loss = 0.10403685\n",
      "Iteration 180, loss = 0.10178475\n",
      "Iteration 181, loss = 0.09944559\n",
      "Iteration 182, loss = 0.09727852\n",
      "Iteration 183, loss = 0.09512300\n",
      "Iteration 184, loss = 0.09309596\n",
      "Iteration 185, loss = 0.09100947\n",
      "Iteration 186, loss = 0.08907918\n",
      "Iteration 187, loss = 0.08713977\n",
      "Iteration 188, loss = 0.08529241\n",
      "Iteration 189, loss = 0.08350807\n",
      "Iteration 190, loss = 0.08175687\n",
      "Iteration 191, loss = 0.08004404\n",
      "Iteration 192, loss = 0.07835844\n",
      "Iteration 193, loss = 0.07672520\n",
      "Iteration 194, loss = 0.07518409\n",
      "Iteration 195, loss = 0.07364996\n",
      "Iteration 196, loss = 0.07214657\n",
      "Iteration 197, loss = 0.07068178\n",
      "Iteration 198, loss = 0.06927846\n",
      "Iteration 199, loss = 0.06790842\n",
      "Iteration 200, loss = 0.06654717\n",
      "\n",
      " The confusion matrix is:\n",
      " [[1 0 0 0 0 0 0 1 0]\n",
      " [0 1 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0 0 0]\n",
      " [0 0 1 0 0 0 2 0 1]\n",
      " [0 0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 0 1]]\n",
      "\n",
      " The accuracy is: 0.6923076923076923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhoolideh.m/anaconda3/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "from scipy.ndimage import convolve\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.utils import shuffle\n",
    "import os.path\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from PIL import Image\n",
    "import glob, os\n",
    "from PIL import Image\n",
    "from resizeimage import resizeimage\n",
    "PATH = 'mlp_model_2Layer_2'\n",
    "clf = joblib.load(PATH).best_estimator_\n",
    "\n",
    "read_dictionary0 = np.load('Featur_0.npy').item()\n",
    "read_dictionary1 = np.load('Featur_1.npy').item()\n",
    "read_dictionary2 = np.load('Featur_2.npy').item()\n",
    "read_dictionary3 = np.load('Featur_3.npy').item()\n",
    "read_dictionary4 = np.load('Featur_4.npy').item()\n",
    "read_dictionary5 = np.load('Featur_5.npy').item()\n",
    "read_dictionary6 = np.load('Featur_6.npy').item()\n",
    "read_dictionary7 = np.load('Featur_7.npy').item()\n",
    "read_dictionary8 = np.load('Featur_8.npy').item()\n",
    "read_dictionary9 = np.load('Featur_9.npy').item()\n",
    "\n",
    "dic_tot = {}\n",
    "for key in read_dictionary0.keys():\n",
    "    dic_tot[key] = np.vstack((read_dictionary0[key][1:], read_dictionary1[key][1:], read_dictionary2[key][1:] , read_dictionary3[key][1:]\\\n",
    "    ,read_dictionary4[key][1:],read_dictionary5[key][1:],read_dictionary6[key][1:],read_dictionary7[key][1:],read_dictionary8[key][1:],read_dictionary9[key][1:] ))\n",
    "\n",
    "\n",
    "\n",
    "read_dictionary0_t = np.load('Featur_0_test.npy').item()\n",
    "read_dictionary1_t = np.load('Featur_1_test.npy').item()\n",
    "read_dictionary2_t = np.load('Featur_2_test.npy').item()\n",
    "read_dictionary3_t = np.load('Featur_3_test.npy').item()\n",
    "read_dictionary4_t = np.load('Featur_4_test.npy').item()\n",
    "read_dictionary5_t = np.load('Featur_5_test.npy').item()\n",
    "read_dictionary6_t = np.load('Featur_6_test.npy').item()\n",
    "read_dictionary7_t = np.load('Featur_7_test.npy').item()\n",
    "read_dictionary8_t = np.load('Featur_8_test.npy').item()\n",
    "read_dictionary9_t = np.load('Featur_9_test.npy').item()\n",
    "\n",
    "dic_tot_test = {}\n",
    "for key in read_dictionary0_t.keys():\n",
    "    dic_tot_test[key] = np.vstack((read_dictionary0_t[key][1:], read_dictionary1_t[key][1:], read_dictionary2_t[key][1:] , read_dictionary3_t[key][1:]\\\n",
    "    ,read_dictionary4_t[key][1:],read_dictionary5_t[key][1:],read_dictionary6_t[key][1:],read_dictionary7_t[key][1:],read_dictionary8_t[key][1:],read_dictionary9_t[key][1:] ))\n",
    "\n",
    "X, y = dic_tot_test['data'], dic_tot_test['label'].ravel()\n",
    "X2, y2 = shuffle(X, y, random_state=0)\n",
    "\n",
    "    \n",
    "X_train, X_test, y_train, y_test = train_test_split(X2 / 255., y2, test_size=0.25)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "prediction = clf.predict(X_test)\n",
    "print('\\n The confusion matrix is:\\n' ,confusion_matrix(y_test,prediction))\n",
    "print( '\\n The accuracy is:', clf.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
